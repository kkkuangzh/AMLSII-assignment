{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model selection\n",
    "\n",
    "A baseline model is proposed first, based on which several modifications are tested to see if the performance can be further improved. These modifications include the location of upsampling layer, patch size, loss function, and using subpixel as upsampling layer. In this notebook, 7 models are trained using 4000 patches and the performances of models are evaluated on the first 10 images of the DIV2K validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import config\n",
    "\n",
    "gpu_devices = config.experimental.list_physical_devices('GPU')\n",
    "\n",
    "for device in gpu_devices: config.experimental.set_memory_growth(device, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load images from directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load training images from directory\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "LR_train_path = './datasets/DIV2K_train_LR_bicubic/X2/'\n",
    "HR_train_path = './datasets/DIV2K_train_HR/'\n",
    "\n",
    "LR_train_imgs = []\n",
    "HR_train_imgs = []\n",
    "\n",
    "for path, subpath, files in os.walk(LR_train_path):\n",
    "    files.sort()\n",
    "    for i in files:\n",
    "        if i == '.DS_Store':\n",
    "            continue\n",
    "        img = Image.open(LR_train_path + i)\n",
    "        LR_train_imgs.append(np.asarray(img))\n",
    "\n",
    "for path, subpath, files in os.walk(HR_train_path):\n",
    "    files.sort()\n",
    "    for i in files:\n",
    "        if i == '.DS_Store':\n",
    "            continue\n",
    "        img = Image.open(HR_train_path + i)\n",
    "        HR_train_imgs.append(np.asarray(img)) \n",
    "\n",
    "print(len(LR_train_imgs))\n",
    "print(len(HR_train_imgs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess (patch extraction + normalization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomly extract pathches from training images (X2 upscaling)\n",
    "\n",
    "from extract_patches import *\n",
    "\n",
    "patch_height = 48\n",
    "patch_width = 48\n",
    "patch_num = 4000\n",
    "up_scale = 2\n",
    "\n",
    "LR_patch_train, HR_patch_train = train_patch(LR_train_imgs, HR_train_imgs, patch_height, patch_width, patch_num, up_scale)\n",
    "\n",
    "\n",
    "print(LR_patch_train.shape)\n",
    "print(HR_patch_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4000, 48, 48, 3)\n",
      "(4000, 96, 96, 3)\n"
     ]
    }
   ],
   "source": [
    "# normaliza imgs from 0~255 to 0~1\n",
    "\n",
    "def normalize(imgs):\n",
    "    return imgs / 255\n",
    "\n",
    "HR_patch_train = normalize(HR_patch_train)\n",
    "LR_patch_train = normalize(LR_patch_train)\n",
    "\n",
    "print(LR_patch_train.shape)\n",
    "print(HR_patch_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build different network architecture for comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# define subpixel layer for up-sampling\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.layers import Lambda\n",
    "\n",
    "def pixelshuffler(input_shape, batch_size, scale=2):\n",
    "    def subpixel_shape(input_shape=input_shape, batch_size=batch_size):\n",
    "        dim = [batch_size,\n",
    "               input_shape[1] * scale,\n",
    "               input_shape[2] * scale,\n",
    "               int(input_shape[3]/ (scale ** 2))]\n",
    "        output_shape = tuple(dim)\n",
    "\n",
    "        return output_shape\n",
    "\n",
    "    def pixelshuffle_upscale(x):\n",
    "        return tf.nn.depth_to_space(input=x, block_size=scale)\n",
    "\n",
    "    return Lambda(function=pixelshuffle_upscale, output_shape=subpixel_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# define baseline model architecture\n",
    "\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import PReLU, Input, Conv2D, UpSampling2D, Dropout, add\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "def res_block(inputs):\n",
    "    x = Conv2D(filters=64, kernel_size=(3, 3), strides=(1, 1), padding='same')(inputs)\n",
    "    #x = BatchNormalization(momentum=0.8)(x)\n",
    "    x = PReLU(shared_axes=[1, 2])(x)\n",
    "    x = Conv2D(filters=64, kernel_size=(3, 3), strides=(1, 1), padding='same')(x)\n",
    "    #x = BatchNormalization(momentum=0.8)(x)\n",
    "    return add([x, inputs])\n",
    "\n",
    "\n",
    "def baseline(patch_height, patch_width, channel, upscale=2):\n",
    "    # conv and then upsample\n",
    "    \n",
    "    inputs = Input(shape=(patch_height, patch_width, channel))\n",
    "    x_init = Conv2D(filters=64, kernel_size=(9, 9), strides=(1, 1), padding='same')(inputs)\n",
    "    x = PReLU(shared_axes=[1, 2])(x_init)\n",
    "    \n",
    "    # residual_block\n",
    "    for i in range(8):\n",
    "        x = res_block(x)\n",
    "        \n",
    "    x = Conv2D(filters=64, kernel_size=(3, 3), strides=(1, 1), padding='same')(x)\n",
    "    x = add([x, x_init])\n",
    "    \n",
    "    # up_block\n",
    "    x = Conv2D(filters=256, kernel_size=(3, 3), strides=(1, 1), padding='same')(x)\n",
    "    x = UpSampling2D(size=(2, 2))(x)  # size:upsampling factor\n",
    "    x = PReLU(shared_axes=[1, 2])(x)\n",
    "    \n",
    "    # output_block\n",
    "    output = Conv2D(filters=3, kernel_size=(9, 9), strides=(1, 1), padding='same')(x)\n",
    "    output = Conv2D(3, (1, 1), activation='sigmoid',padding='same')(output)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=output)\n",
    "    \n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modify the location of upsampling layer\n",
    "# define pre-upsampling network architecture\n",
    "\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import PReLU, Input, Conv2D, UpSampling2D, Dropout, add\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "def res_block(inputs):\n",
    "    x = Conv2D(filters=64, kernel_size=(3, 3), strides=(1, 1), padding='same')(inputs)\n",
    "    x = PReLU(shared_axes=[1, 2])(x)\n",
    "    x = Conv2D(filters=64, kernel_size=(3, 3), strides=(1, 1), padding='same')(x)\n",
    "    return add([x, inputs])\n",
    "\n",
    "def model1(patch_height, patch_width, channel, upscale=2):\n",
    "    # upsample and then conv\n",
    "    \n",
    "    inputs = Input(shape=(patch_height, patch_width, channel))\n",
    "    x_init = Conv2D(filters=64, kernel_size=(9, 9), strides=(1, 1), padding='same')(inputs)\n",
    "    x = PReLU(shared_axes=[1, 2])(x_init)\n",
    "    \n",
    "    x = Conv2D(filters=64, kernel_size=(3, 3), strides=(1, 1), padding='same')(x)\n",
    "    x = add([x, x_init])\n",
    "    \n",
    "    # up_block\n",
    "    x = Conv2D(filters=128, kernel_size=(3, 3), strides=(1, 1), padding='same')(x)\n",
    "    x = Conv2D(filters=64, kernel_size=(3, 3), strides=(1, 1), padding='same')(x)\n",
    "    x = UpSampling2D(size=(2, 2))(x)\n",
    "    x = PReLU(shared_axes=[1, 2])(x)\n",
    "    \n",
    "    # residual_block\n",
    "    for i in range(8):\n",
    "        x = res_block(x)\n",
    "    \n",
    "    # output_block\n",
    "    output = Conv2D(filters=3, kernel_size=(9, 9), strides=(1, 1), padding='same')(x)\n",
    "    output = Conv2D(3, (1, 1), activation='sigmoid',padding='same')(output)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=output)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modify upsampling layer to subpixel layer\n",
    "\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import PReLU, Input, Conv2D, UpSampling2D, Dropout, add\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "def res_block(inputs):\n",
    "    x = Conv2D(filters=64, kernel_size=(3, 3), strides=(1, 1), padding='same')(inputs)\n",
    "    x = PReLU(shared_axes=[1, 2])(x)\n",
    "    x = Conv2D(filters=64, kernel_size=(3, 3), strides=(1, 1), padding='same')(x)\n",
    "    return add([x, inputs])\n",
    "\n",
    "\n",
    "def model2(patch_height, patch_width, channel, upscale=2):\n",
    "    # conv and then upsample\n",
    "    \n",
    "    inputs = Input(shape=(patch_height, patch_width, channel))\n",
    "    x_init = Conv2D(filters=64, kernel_size=(9, 9), strides=(1, 1), padding='same')(inputs)\n",
    "    x = PReLU(shared_axes=[1, 2])(x_init)\n",
    "    \n",
    "    # residual_block\n",
    "    for i in range(8):\n",
    "        x = res_block(x)\n",
    "        \n",
    "    x = Conv2D(filters=64, kernel_size=(3, 3), strides=(1, 1), padding='same')(x)\n",
    "    x = add([x, x_init])\n",
    "    \n",
    "    # up_block\n",
    "    x = Conv2D(filters=256, kernel_size=(3, 3), strides=(1, 1), padding='same')(x)\n",
    "    x = pixelshuffler(input_shape=(48,48,3), batch_size=4, scale=upscale)(x)\n",
    "    x = PReLU(shared_axes=[1, 2])(x)\n",
    "    \n",
    "    # output_block\n",
    "    output = Conv2D(filters=3, kernel_size=(9, 9), strides=(1, 1), padding='same')(x)\n",
    "    output = Conv2D(3, (1, 1), activation='sigmoid',padding='same')(output)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=output)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define perceptual loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define perceptual loss based on the first 5 layers of VGG19 model\n",
    "\n",
    "from keras.applications.vgg19 import VGG19\n",
    "from keras.layers import Input, Lambda\n",
    "import keras\n",
    "\n",
    "# get VGG network\n",
    "def get_VGG19(input_size):\n",
    "    \n",
    "    vgg_input = Input(input_size)\n",
    "    vgg = VGG19(include_top=False, input_tensor=vgg_input)\n",
    "    for l in vgg.layers: \n",
    "        l.trainable = False\n",
    "    vgg_output = vgg.get_layer('block2_conv2').output\n",
    "    \n",
    "    return vgg_input, vgg_output\n",
    "\n",
    "def perceptual_loss(y_true, y_pred):\n",
    "    \n",
    "    y_true = vgg_content(y_true)\n",
    "    y_predict = vgg_content(y_pred)\n",
    "    loss = keras.losses.mean_squared_error(y_true, y_predict)\n",
    "    \n",
    "    return loss\n",
    "\n",
    "\n",
    "vgg_input, vgg_output = get_VGG19(input_size=(96,96,3))\n",
    "vgg_content = Model(vgg_input, vgg_output)\n",
    "#vgg_content.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train 7 models for 10 epochs (batch_size=4, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3200 samples, validate on 800 samples\n",
      "Epoch 1/10\n",
      "3200/3200 [==============================] - 26s 8ms/step - loss: 0.0103 - accuracy: 0.7556 - val_loss: 0.0034 - val_accuracy: 0.7749\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.00338, saving model to ./model_selection/baseline.h5\n",
      "Epoch 2/10\n",
      "3200/3200 [==============================] - 24s 8ms/step - loss: 0.0030 - accuracy: 0.8329 - val_loss: 0.0023 - val_accuracy: 0.8366\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.00338 to 0.00231, saving model to ./model_selection/baseline.h5\n",
      "Epoch 3/10\n",
      "3200/3200 [==============================] - 25s 8ms/step - loss: 0.0023 - accuracy: 0.8452 - val_loss: 0.0018 - val_accuracy: 0.8618\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.00231 to 0.00184, saving model to ./model_selection/baseline.h5\n",
      "Epoch 4/10\n",
      "3200/3200 [==============================] - 22s 7ms/step - loss: 0.0019 - accuracy: 0.8620 - val_loss: 0.0017 - val_accuracy: 0.8545\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.00184 to 0.00168, saving model to ./model_selection/baseline.h5\n",
      "Epoch 5/10\n",
      "3200/3200 [==============================] - 23s 7ms/step - loss: 0.0018 - accuracy: 0.8678 - val_loss: 0.0015 - val_accuracy: 0.8748\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.00168 to 0.00153, saving model to ./model_selection/baseline.h5\n",
      "Epoch 6/10\n",
      "3200/3200 [==============================] - 25s 8ms/step - loss: 0.0016 - accuracy: 0.8756 - val_loss: 0.0014 - val_accuracy: 0.8685\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.00153 to 0.00144, saving model to ./model_selection/baseline.h5\n",
      "Epoch 7/10\n",
      "3200/3200 [==============================] - 25s 8ms/step - loss: 0.0015 - accuracy: 0.8757 - val_loss: 0.0015 - val_accuracy: 0.8250\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.00144\n",
      "Epoch 8/10\n",
      "3200/3200 [==============================] - 24s 8ms/step - loss: 0.0016 - accuracy: 0.8796 - val_loss: 0.0013 - val_accuracy: 0.8776\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.00144 to 0.00131, saving model to ./model_selection/baseline.h5\n",
      "Epoch 9/10\n",
      "3200/3200 [==============================] - 25s 8ms/step - loss: 0.0014 - accuracy: 0.8862 - val_loss: 0.0013 - val_accuracy: 0.8701\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.00131\n",
      "Epoch 10/10\n",
      "3200/3200 [==============================] - 25s 8ms/step - loss: 0.0013 - accuracy: 0.8941 - val_loss: 0.0012 - val_accuracy: 0.8967\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.00131 to 0.00125, saving model to ./model_selection/baseline.h5\n"
     ]
    }
   ],
   "source": [
    "# baaseline model\n",
    "\n",
    "model = baseline(48, 48, 3)\n",
    "model.compile(optimizer=Adam(lr=1e-4), loss='mse', metrics=['accuracy'])\n",
    "checkpointer = ModelCheckpoint(filepath='./model_selection/baseline.h5', verbose=1, \n",
    "                               monitor='val_loss', mode='auto', save_best_only=True)\n",
    "\n",
    "history = model.fit(LR_patch_train, HR_patch_train, epochs=10, verbose=1, \n",
    "                    batch_size=4, validation_split=0.2,\n",
    "                    callbacks=[checkpointer]\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3200 samples, validate on 800 samples\n",
      "Epoch 1/10\n",
      "3200/3200 [==============================] - 34s 11ms/step - loss: 0.0110 - accuracy: 0.7201 - val_loss: 0.0051 - val_accuracy: 0.7401\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.00507, saving model to ./model_selection/architecture.h5\n",
      "Epoch 2/10\n",
      "3200/3200 [==============================] - 33s 10ms/step - loss: 0.0033 - accuracy: 0.8020 - val_loss: 0.0024 - val_accuracy: 0.8248\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.00507 to 0.00245, saving model to ./model_selection/architecture.h5\n",
      "Epoch 3/10\n",
      "3200/3200 [==============================] - 30s 9ms/step - loss: 0.0029 - accuracy: 0.8095 - val_loss: 0.0020 - val_accuracy: 0.8505\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.00245 to 0.00198, saving model to ./model_selection/architecture.h5\n",
      "Epoch 4/10\n",
      "3200/3200 [==============================] - 30s 9ms/step - loss: 0.0021 - accuracy: 0.8350 - val_loss: 0.0019 - val_accuracy: 0.8215\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.00198 to 0.00194, saving model to ./model_selection/architecture.h5\n",
      "Epoch 5/10\n",
      "3200/3200 [==============================] - 32s 10ms/step - loss: 0.0020 - accuracy: 0.8401 - val_loss: 0.0019 - val_accuracy: 0.8107\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.00194 to 0.00187, saving model to ./model_selection/architecture.h5\n",
      "Epoch 6/10\n",
      "3200/3200 [==============================] - 33s 10ms/step - loss: 0.0020 - accuracy: 0.8273 - val_loss: 0.0019 - val_accuracy: 0.8428\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.00187\n",
      "Epoch 7/10\n",
      "3200/3200 [==============================] - 32s 10ms/step - loss: 0.0017 - accuracy: 0.8544 - val_loss: 0.0016 - val_accuracy: 0.8775\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.00187 to 0.00158, saving model to ./model_selection/architecture.h5\n",
      "Epoch 8/10\n",
      "3200/3200 [==============================] - 33s 10ms/step - loss: 0.0016 - accuracy: 0.8583 - val_loss: 0.0016 - val_accuracy: 0.8641\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.00158 to 0.00156, saving model to ./model_selection/architecture.h5\n",
      "Epoch 9/10\n",
      "3200/3200 [==============================] - 32s 10ms/step - loss: 0.0015 - accuracy: 0.8678 - val_loss: 0.0013 - val_accuracy: 0.8638\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.00156 to 0.00130, saving model to ./model_selection/architecture.h5\n",
      "Epoch 10/10\n",
      "3200/3200 [==============================] - 33s 10ms/step - loss: 0.0014 - accuracy: 0.8728 - val_loss: 0.0013 - val_accuracy: 0.8922\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.00130 to 0.00129, saving model to ./model_selection/architecture.h5\n"
     ]
    }
   ],
   "source": [
    "# pre-upsampling network\n",
    "\n",
    "model = model1(48, 48, 3)\n",
    "model.compile(optimizer=Adam(lr=1e-4), loss='mse', metrics=['accuracy'])\n",
    "checkpointer = ModelCheckpoint(filepath='./model_selection/architecture.h5', verbose=1, \n",
    "                               monitor='val_loss', mode='auto', save_best_only=True)\n",
    "\n",
    "history = model.fit(LR_patch_train, HR_patch_train, epochs=10, verbose=1, \n",
    "                    batch_size=4, validation_split=0.2,\n",
    "                    callbacks=[checkpointer]\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3200 samples, validate on 800 samples\n",
      "Epoch 1/10\n",
      "3200/3200 [==============================] - 45s 14ms/step - loss: 0.7253 - accuracy: 0.7476 - val_loss: 0.4380 - val_accuracy: 0.8063\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.43804, saving model to ./model_selection/loss_function.h5\n",
      "Epoch 2/10\n",
      "3200/3200 [==============================] - 34s 11ms/step - loss: 0.4106 - accuracy: 0.8282 - val_loss: 0.3627 - val_accuracy: 0.8697\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.43804 to 0.36274, saving model to ./model_selection/loss_function.h5\n",
      "Epoch 3/10\n",
      "3200/3200 [==============================] - 34s 11ms/step - loss: 0.3699 - accuracy: 0.8465 - val_loss: 0.3396 - val_accuracy: 0.8185\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.36274 to 0.33961, saving model to ./model_selection/loss_function.h5\n",
      "Epoch 4/10\n",
      "3200/3200 [==============================] - 35s 11ms/step - loss: 0.3458 - accuracy: 0.8501 - val_loss: 0.3291 - val_accuracy: 0.8426\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.33961 to 0.32908, saving model to ./model_selection/loss_function.h5\n",
      "Epoch 5/10\n",
      "3200/3200 [==============================] - 35s 11ms/step - loss: 0.3277 - accuracy: 0.8574 - val_loss: 0.3274 - val_accuracy: 0.8643\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.32908 to 0.32740, saving model to ./model_selection/loss_function.h5\n",
      "Epoch 6/10\n",
      "3200/3200 [==============================] - 35s 11ms/step - loss: 0.3164 - accuracy: 0.8595 - val_loss: 0.3307 - val_accuracy: 0.8705\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.32740\n",
      "Epoch 7/10\n",
      "3200/3200 [==============================] - 35s 11ms/step - loss: 0.3082 - accuracy: 0.8648 - val_loss: 0.2994 - val_accuracy: 0.8297\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.32740 to 0.29939, saving model to ./model_selection/loss_function.h5\n",
      "Epoch 8/10\n",
      "3200/3200 [==============================] - 35s 11ms/step - loss: 0.3012 - accuracy: 0.8652 - val_loss: 0.2972 - val_accuracy: 0.8442\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.29939 to 0.29723, saving model to ./model_selection/loss_function.h5\n",
      "Epoch 9/10\n",
      "3200/3200 [==============================] - 35s 11ms/step - loss: 0.2958 - accuracy: 0.8701 - val_loss: 0.2932 - val_accuracy: 0.8528\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.29723 to 0.29316, saving model to ./model_selection/loss_function.h5\n",
      "Epoch 10/10\n",
      "3200/3200 [==============================] - 35s 11ms/step - loss: 0.2908 - accuracy: 0.8764 - val_loss: 0.2939 - val_accuracy: 0.8799\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.29316\n"
     ]
    }
   ],
   "source": [
    "# change loss function from MSE to defined perceptual loss\n",
    "\n",
    "model = baseline(48, 48, 3)\n",
    "model.compile(optimizer=Adam(lr=1e-4), loss=perceptual_loss, metrics=['accuracy'])\n",
    "checkpointer = ModelCheckpoint(filepath='./model_selection/loss_function.h5', verbose=1, \n",
    "                               monitor='val_loss', mode='auto', save_best_only=True)\n",
    "\n",
    "history = model.fit(LR_patch_train, HR_patch_train, epochs=10, verbose=1, \n",
    "                    batch_size=4, validation_split=0.2,\n",
    "                    callbacks=[checkpointer]\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3200 samples, validate on 800 samples\n",
      "Epoch 1/10\n",
      "3200/3200 [==============================] - 21s 7ms/step - loss: 0.0125 - accuracy: 0.7130 - val_loss: 0.0041 - val_accuracy: 0.7911\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.00408, saving model to ./model_selection/subpixel_up1.h5\n",
      "Epoch 2/10\n",
      "3200/3200 [==============================] - 19s 6ms/step - loss: 0.0039 - accuracy: 0.7918 - val_loss: 0.0038 - val_accuracy: 0.6356\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.00408 to 0.00379, saving model to ./model_selection/subpixel_up1.h5\n",
      "Epoch 3/10\n",
      "3200/3200 [==============================] - 19s 6ms/step - loss: 0.0028 - accuracy: 0.8159 - val_loss: 0.0023 - val_accuracy: 0.8446\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.00379 to 0.00234, saving model to ./model_selection/subpixel_up1.h5\n",
      "Epoch 4/10\n",
      "3200/3200 [==============================] - 19s 6ms/step - loss: 0.0023 - accuracy: 0.8372 - val_loss: 0.0022 - val_accuracy: 0.8431\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.00234 to 0.00218, saving model to ./model_selection/subpixel_up1.h5\n",
      "Epoch 5/10\n",
      "3200/3200 [==============================] - 19s 6ms/step - loss: 0.0019 - accuracy: 0.8493 - val_loss: 0.0017 - val_accuracy: 0.8678\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.00218 to 0.00175, saving model to ./model_selection/subpixel_up1.h5\n",
      "Epoch 6/10\n",
      "3200/3200 [==============================] - 20s 6ms/step - loss: 0.0018 - accuracy: 0.8585 - val_loss: 0.0017 - val_accuracy: 0.8401\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.00175 to 0.00170, saving model to ./model_selection/subpixel_up1.h5\n",
      "Epoch 7/10\n",
      "3200/3200 [==============================] - 18s 6ms/step - loss: 0.0017 - accuracy: 0.8590 - val_loss: 0.0014 - val_accuracy: 0.8803\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.00170 to 0.00145, saving model to ./model_selection/subpixel_up1.h5\n",
      "Epoch 8/10\n",
      "3200/3200 [==============================] - 20s 6ms/step - loss: 0.0015 - accuracy: 0.8745 - val_loss: 0.0014 - val_accuracy: 0.8521\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.00145 to 0.00139, saving model to ./model_selection/subpixel_up1.h5\n",
      "Epoch 9/10\n",
      "3200/3200 [==============================] - 20s 6ms/step - loss: 0.0015 - accuracy: 0.8773 - val_loss: 0.0013 - val_accuracy: 0.8976\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.00139 to 0.00133, saving model to ./model_selection/subpixel_up1.h5\n",
      "Epoch 10/10\n",
      "3200/3200 [==============================] - 20s 6ms/step - loss: 0.0014 - accuracy: 0.8835 - val_loss: 0.0013 - val_accuracy: 0.8823\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.00133\n"
     ]
    }
   ],
   "source": [
    "# modify the upsampling layer to subpixel layer\n",
    "\n",
    "model = model2(48, 48, 3)\n",
    "model.compile(optimizer=Adam(lr=1e-4), loss='mse', metrics=['accuracy'])\n",
    "checkpointer = ModelCheckpoint(filepath='./model_selection/subpixel_up1.h5', verbose=1, \n",
    "                               monitor='val_loss', mode='auto', save_best_only=True)\n",
    "\n",
    "history = model.fit(LR_patch_train, HR_patch_train, epochs=10, verbose=1, \n",
    "                    batch_size=4, validation_split=0.2,\n",
    "                    callbacks=[checkpointer]\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3200 samples, validate on 800 samples\n",
      "Epoch 1/10\n",
      "3200/3200 [==============================] - 21s 7ms/step - loss: 0.0108 - accuracy: 0.7538 - val_loss: 0.0037 - val_accuracy: 0.7852\n",
      "Epoch 2/10\n",
      "3200/3200 [==============================] - 17s 5ms/step - loss: 0.0035 - accuracy: 0.8402 - val_loss: 0.0024 - val_accuracy: 0.8519\n",
      "Epoch 3/10\n",
      "3200/3200 [==============================] - 17s 5ms/step - loss: 0.0026 - accuracy: 0.8532 - val_loss: 0.0021 - val_accuracy: 0.8218\n",
      "Epoch 4/10\n",
      "3200/3200 [==============================] - 17s 5ms/step - loss: 0.0023 - accuracy: 0.8554 - val_loss: 0.0017 - val_accuracy: 0.8716\n",
      "Epoch 5/10\n",
      "3200/3200 [==============================] - 17s 5ms/step - loss: 0.0019 - accuracy: 0.8751 - val_loss: 0.0015 - val_accuracy: 0.8789\n",
      "Epoch 6/10\n",
      "3200/3200 [==============================] - 18s 6ms/step - loss: 0.0018 - accuracy: 0.8766 - val_loss: 0.0015 - val_accuracy: 0.8588\n",
      "Epoch 7/10\n",
      "3200/3200 [==============================] - 18s 6ms/step - loss: 0.0016 - accuracy: 0.8811 - val_loss: 0.0014 - val_accuracy: 0.8932\n",
      "Epoch 8/10\n",
      "3200/3200 [==============================] - 17s 5ms/step - loss: 0.0015 - accuracy: 0.8905 - val_loss: 0.0014 - val_accuracy: 0.8707\n",
      "Epoch 9/10\n",
      "3200/3200 [==============================] - 18s 6ms/step - loss: 0.0015 - accuracy: 0.8800 - val_loss: 0.0014 - val_accuracy: 0.8724\n",
      "Epoch 10/10\n",
      "3200/3200 [==============================] - 18s 6ms/step - loss: 0.0014 - accuracy: 0.8951 - val_loss: 0.0012 - val_accuracy: 0.9063\n"
     ]
    }
   ],
   "source": [
    "# input patch size = 16*16\n",
    "\n",
    "model = baseline(16, 16, 3)\n",
    "model.compile(optimizer=Adam(lr=1e-4), loss='mse', metrics=['accuracy'])\n",
    "#checkpointer = ModelCheckpoint(filepath='./model_selection/baseline1.h5', verbose=1, \n",
    "#                               monitor='val_loss', mode='auto', save_best_only=True)\n",
    "\n",
    "history = model.fit(LR_patch_train, HR_patch_train, epochs=10, verbose=1, \n",
    "                    batch_size=4, validation_split=0.2,\n",
    "                    #callbacks=[checkpointer]\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3200 samples, validate on 800 samples\n",
      "Epoch 1/10\n",
      "3200/3200 [==============================] - 24s 7ms/step - loss: 0.0162 - accuracy: 0.6797 - val_loss: 0.0048 - val_accuracy: 0.7484\n",
      "Epoch 2/10\n",
      "3200/3200 [==============================] - 19s 6ms/step - loss: 0.0048 - accuracy: 0.7751 - val_loss: 0.0034 - val_accuracy: 0.8061\n",
      "Epoch 3/10\n",
      "3200/3200 [==============================] - 21s 7ms/step - loss: 0.0037 - accuracy: 0.8058 - val_loss: 0.0026 - val_accuracy: 0.8238\n",
      "Epoch 4/10\n",
      "3200/3200 [==============================] - 21s 7ms/step - loss: 0.0028 - accuracy: 0.8372 - val_loss: 0.0025 - val_accuracy: 0.8327\n",
      "Epoch 5/10\n",
      "3200/3200 [==============================] - 20s 6ms/step - loss: 0.0026 - accuracy: 0.8307 - val_loss: 0.0019 - val_accuracy: 0.8662\n",
      "Epoch 6/10\n",
      "3200/3200 [==============================] - 19s 6ms/step - loss: 0.0021 - accuracy: 0.8469 - val_loss: 0.0017 - val_accuracy: 0.8668\n",
      "Epoch 7/10\n",
      "3200/3200 [==============================] - 20s 6ms/step - loss: 0.0019 - accuracy: 0.8534 - val_loss: 0.0014 - val_accuracy: 0.8723\n",
      "Epoch 8/10\n",
      "3200/3200 [==============================] - 21s 7ms/step - loss: 0.0018 - accuracy: 0.8581 - val_loss: 0.0014 - val_accuracy: 0.8721\n",
      "Epoch 9/10\n",
      "3200/3200 [==============================] - 20s 6ms/step - loss: 0.0016 - accuracy: 0.8669 - val_loss: 0.0014 - val_accuracy: 0.8738\n",
      "Epoch 10/10\n",
      "3200/3200 [==============================] - 21s 6ms/step - loss: 0.0016 - accuracy: 0.8660 - val_loss: 0.0013 - val_accuracy: 0.8380\n"
     ]
    }
   ],
   "source": [
    "# input patch size = 32*32\n",
    "\n",
    "model = baseline(32, 32, 3)\n",
    "model.compile(optimizer=Adam(lr=1e-4), loss='mse', metrics=['accuracy'])\n",
    "#checkpointer = ModelCheckpoint(filepath='./model_selection/patch3232.h5', verbose=1, \n",
    "#                               monitor='val_loss', mode='auto', save_best_only=True)\n",
    "\n",
    "history = model.fit(LR_patch_train, HR_patch_train, epochs=10, verbose=1, \n",
    "                    batch_size=4, validation_split=0.2,\n",
    "                    #callbacks=[checkpointer]\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3200 samples, validate on 800 samples\n",
      "Epoch 1/10\n",
      "3200/3200 [==============================] - 45s 14ms/step - loss: 0.0096 - accuracy: 0.7904 - val_loss: 0.0033 - val_accuracy: 0.8312\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.00330, saving model to ./model_selection/patch6464_model.h5\n",
      "Epoch 2/10\n",
      "3200/3200 [==============================] - 41s 13ms/step - loss: 0.0030 - accuracy: 0.8454 - val_loss: 0.0023 - val_accuracy: 0.8358\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.00330 to 0.00226, saving model to ./model_selection/patch6464_model.h5\n",
      "Epoch 3/10\n",
      "3200/3200 [==============================] - 41s 13ms/step - loss: 0.0022 - accuracy: 0.8628 - val_loss: 0.0025 - val_accuracy: 0.8833\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.00226\n",
      "Epoch 4/10\n",
      "3200/3200 [==============================] - 41s 13ms/step - loss: 0.0019 - accuracy: 0.8712 - val_loss: 0.0021 - val_accuracy: 0.8181\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.00226 to 0.00210, saving model to ./model_selection/patch6464_model.h5\n",
      "Epoch 5/10\n",
      "3200/3200 [==============================] - 39s 12ms/step - loss: 0.0018 - accuracy: 0.8708 - val_loss: 0.0015 - val_accuracy: 0.8917\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.00210 to 0.00147, saving model to ./model_selection/patch6464_model.h5\n",
      "Epoch 6/10\n",
      "3200/3200 [==============================] - 40s 12ms/step - loss: 0.0017 - accuracy: 0.8822 - val_loss: 0.0017 - val_accuracy: 0.8421\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.00147\n",
      "Epoch 7/10\n",
      "3200/3200 [==============================] - 39s 12ms/step - loss: 0.0015 - accuracy: 0.8918 - val_loss: 0.0013 - val_accuracy: 0.8815\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.00147 to 0.00134, saving model to ./model_selection/patch6464_model.h5\n",
      "Epoch 8/10\n",
      "3200/3200 [==============================] - 41s 13ms/step - loss: 0.0015 - accuracy: 0.8856 - val_loss: 0.0024 - val_accuracy: 0.8346\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.00134\n",
      "Epoch 9/10\n",
      "3200/3200 [==============================] - 41s 13ms/step - loss: 0.0014 - accuracy: 0.8935 - val_loss: 0.0012 - val_accuracy: 0.9009\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.00134 to 0.00123, saving model to ./model_selection/patch6464_model.h5\n",
      "Epoch 10/10\n",
      "3200/3200 [==============================] - 43s 13ms/step - loss: 0.0013 - accuracy: 0.8966 - val_loss: 0.0012 - val_accuracy: 0.9026\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.00123 to 0.00121, saving model to ./model_selection/patch6464_model.h5\n"
     ]
    }
   ],
   "source": [
    "# input patch size = 64*64\n",
    "\n",
    "model = baseline(64, 64, 3)\n",
    "model.compile(optimizer=Adam(lr=1e-4), loss='mse', metrics=['accuracy'])\n",
    "checkpointer = ModelCheckpoint(filepath='./model_selection/patch6464_model.h5', verbose=1, \n",
    "                               monitor='val_loss', mode='auto', save_best_only=True)\n",
    "\n",
    "history = model.fit(LR_patch_train, HR_patch_train, epochs=10, verbose=1, \n",
    "                    batch_size=4, validation_split=0.2,\n",
    "                    callbacks=[checkpointer]\n",
    "                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare performance on the first 10 test images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load test images from directory\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "LR_train_path = './datasets/DIV2K_valid_LR_bicubic/X2/'\n",
    "HR_train_path = './datasets/DIV2K_valid_HR//'\n",
    "\n",
    "LR_train_imgs = []\n",
    "HR_train_imgs = []\n",
    "\n",
    "for path, subpath, files in os.walk(LR_train_path):\n",
    "    files.sort()\n",
    "    for i in files:\n",
    "        if i == '.DS_Store':\n",
    "            continue\n",
    "        img = Image.open(LR_train_path + i)\n",
    "        LR_train_imgs.append(np.asarray(img))\n",
    "\n",
    "for path, subpath, files in os.walk(HR_train_path):\n",
    "    files.sort()\n",
    "    for i in files:\n",
    "        if i == '.DS_Store':\n",
    "            continue\n",
    "        img = Image.open(HR_train_path + i)\n",
    "        HR_train_imgs.append(np.asarray(img)) \n",
    "\n",
    "        \n",
    "LR_valid_imgs = LR_valid_imgs[:10]\n",
    "HR_valid_imgs = HR_valid_imgs[:10]\n",
    "\n",
    "print(len(LR_valid_imgs))\n",
    "print(len(HR_valid_imgs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load trained models\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.models import load_model\n",
    "\n",
    "model = load_model('./model_selection/baseline.h5', custom_objects={'tf': tf})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize images for predicting\n",
    "\n",
    "def normalize(imgs):\n",
    "    return imgs / 255\n",
    "\n",
    "def denormalize(imgs):\n",
    "    imgs = imgs * 255\n",
    "    return imgs.astype(np.uint8)\n",
    "\n",
    "for i in range(len(LR_valid_imgs)):\n",
    "    LR_valid_imgs[i] = normalize(LR_valid_imgs[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use trained model to predict test images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict and reconstruct test images\n",
    "# stride should be smaller than patch size to cover all the pixels\n",
    "\n",
    "import time\n",
    "from extract_patches import *\n",
    "\n",
    "test_num = 10\n",
    "patch_height = 48\n",
    "patch_width = 48\n",
    "stride = 40\n",
    "up_scale = 2\n",
    "\n",
    "time_start=time.time()\n",
    "\n",
    "predicted_HR_list = test_patch(LR_valid_imgs, test_num, patch_height, patch_width, stride, model, up_scale)\n",
    "\n",
    "time_end=time.time()\n",
    "print('Time cost to predict: ', time_end-time_start, 's')\n",
    "\n",
    "for i in range(len(predicted_HR_list)):\n",
    "    predicted_HR_list[i] = denormalize(predicted_HR_list[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare with HR images \n",
    "# calculate PSNR(peak_signal_noise_ratio) and SSIM(structural_similarity) metrics\n",
    "\n",
    "from skimage.metrics import peak_signal_noise_ratio, structural_similarity\n",
    "\n",
    "PSNR_val = []\n",
    "SSIM_val = []\n",
    "\n",
    "for i in range(len(predicted_HR_list)):\n",
    "    PSNR = peak_signal_noise_ratio(HR_valid_imgs[i], predicted_HR_list[i])\n",
    "    SSIM = structural_similarity(HR_valid_imgs[i], predicted_HR_list[i], multichannel=True)\n",
    "    PSNR_val.append(PSNR)\n",
    "    SSIM_val.append(SSIM)\n",
    "\n",
    "print('PSNR: ', PSNR_val)\n",
    "print('SSIM: ', SSIM_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
