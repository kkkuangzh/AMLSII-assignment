{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model training (X2 upscaling)\n",
    "After deciding the final network architecture, more patches and epochs are used to train a better model. In this notebook, 12000 training patches are extracted to train the X2 model at scale 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import config\n",
    "\n",
    "gpu_devices = config.experimental.list_physical_devices('GPU')\n",
    "\n",
    "for device in gpu_devices: config.experimental.set_memory_growth(device, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load training images from directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load training images from directory\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "# LR_train_path = './datasets/DIV2K_train_LR_unknown/X2/'\n",
    "LR_train_path = './datasets/DIV2K_train_LR_bicubic/X2/'\n",
    "HR_train_path = './datasets/DIV2K_train_HR/'\n",
    "\n",
    "LR_train_imgs = []\n",
    "HR_train_imgs = []\n",
    "\n",
    "for path, subpath, files in os.walk(LR_train_path):\n",
    "    files.sort()\n",
    "    for i in files:\n",
    "        if i == '.DS_Store':\n",
    "            continue\n",
    "        img = Image.open(LR_train_path + i)\n",
    "        LR_train_imgs.append(np.asarray(img))\n",
    "\n",
    "for path, subpath, files in os.walk(HR_train_path):\n",
    "    files.sort()\n",
    "    for i in files:\n",
    "        if i == '.DS_Store':\n",
    "            continue\n",
    "        img = Image.open(HR_train_path + i)\n",
    "        HR_train_imgs.append(np.asarray(img)) \n",
    "\n",
    "print(len(LR_train_imgs))\n",
    "print(len(HR_train_imgs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess (patch extraction + normalization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12000, 48, 48, 3)\n",
      "(12000, 96, 96, 3)\n",
      "(48, 48, 3)\n",
      "(96, 96, 3)\n"
     ]
    }
   ],
   "source": [
    "# randomly extract pathches from training images (X2 upscaling)\n",
    "\n",
    "from extract_patches import *\n",
    "\n",
    "patch_height = 48\n",
    "patch_width = 48\n",
    "patch_num = 12000\n",
    "up_scale = 2\n",
    "\n",
    "LR_patch_train, HR_patch_train = train_patch(LR_train_imgs, HR_train_imgs, patch_height, patch_width, patch_num, up_scale)\n",
    "\n",
    "\n",
    "print(LR_patch_train.shape)\n",
    "print(HR_patch_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12000, 48, 48, 3)\n",
      "(12000, 96, 96, 3)\n"
     ]
    }
   ],
   "source": [
    "# normaliza imgs from 0~255 to 0~1\n",
    "\n",
    "def normalize(imgs):\n",
    "    return imgs / 255\n",
    "\n",
    "HR_patch_train = normalize(HR_patch_train)\n",
    "LR_patch_train = normalize(LR_patch_train)\n",
    "\n",
    "print(LR_patch_train.shape)\n",
    "print(HR_patch_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Network Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# define subpixel layer\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.layers import Lambda\n",
    "\n",
    "def pixelshuffler(input_shape, batch_size, scale=2):\n",
    "    def subpixel_shape(input_shape=input_shape, batch_size=batch_size):\n",
    "        dim = [batch_size,\n",
    "               input_shape[1] * scale,\n",
    "               input_shape[2] * scale,\n",
    "               int(input_shape[3]/ (scale ** 2))]\n",
    "\n",
    "        output_shape = tuple(dim)\n",
    "\n",
    "        return output_shape\n",
    "\n",
    "    def pixelshuffle_upscale(x):\n",
    "        return tf.nn.depth_to_space(input=x, block_size=scale)\n",
    "\n",
    "    return Lambda(function=pixelshuffle_upscale, output_shape=subpixel_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model architecture\n",
    "\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import PReLU, Input, Conv2D, add\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "def res_block(inputs):\n",
    "    x = Conv2D(filters=64, kernel_size=(3, 3), strides=(1, 1), padding='same')(inputs)\n",
    "    x = PReLU(shared_axes=[1, 2])(x)\n",
    "    x = Conv2D(filters=64, kernel_size=(3, 3), strides=(1, 1), padding='same')(x)\n",
    "    return add([x, inputs])\n",
    "\n",
    "\n",
    "def final_model(patch_height, patch_width, channel, upscale=2):\n",
    "\n",
    "    inputs = Input(shape=(patch_height, patch_width, channel))\n",
    "    x_init = Conv2D(filters=64, kernel_size=(9, 9), strides=(1, 1), padding='same')(inputs)\n",
    "    x = PReLU(shared_axes=[1, 2])(x_init)\n",
    "    \n",
    "    # residual block\n",
    "    for i in range(8):\n",
    "        x = res_block(x)\n",
    "        \n",
    "    x = Conv2D(filters=64, kernel_size=(3, 3), strides=(1, 1), padding='same')(x)\n",
    "    x = add([x, x_init])\n",
    "    \n",
    "    # sub-pixel up_block\n",
    "    x = Conv2D(filters=256, kernel_size=(3, 3), strides=(1, 1), padding='same')(x)\n",
    "    x = pixelshuffler(input_shape=(48,48,3), batch_size=4, scale=upscale)(x)\n",
    "    x = PReLU(shared_axes=[1, 2])(x)\n",
    "    \n",
    "    # output_block\n",
    "    output = Conv2D(filters=3, kernel_size=(9, 9), strides=(1, 1), padding='same')(x)\n",
    "    output = Conv2D(3, (1, 1), activation='sigmoid',padding='same')(output)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=output)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define perceptual loss based on the first 5 layers of VGG19 model\n",
    "# compare the X2 model output of size (96, 96)\n",
    "\n",
    "from keras.applications.vgg19 import VGG19\n",
    "from keras.layers import Input, Lambda\n",
    "import keras\n",
    "\n",
    "# get VGG network\n",
    "def get_VGG19(input_size):\n",
    "    \n",
    "    vgg_input = Input(input_size)\n",
    "    vgg = VGG19(include_top=False, input_tensor=vgg_input)\n",
    "    for l in vgg.layers: \n",
    "        l.trainable = False\n",
    "    vgg_output = vgg.get_layer('block2_conv2').output\n",
    "    \n",
    "    return vgg_input, vgg_output\n",
    "\n",
    "def perceptual_loss_x2(y_true, y_pred):\n",
    "    \n",
    "    y_t = vgg_content(y_true)\n",
    "    y_p = vgg_content(y_pred)\n",
    "    loss = keras.losses.mean_squared_error(y_t, y_p)\n",
    "    \n",
    "    return loss\n",
    "\n",
    "# VGG input = model output\n",
    "vgg_input, vgg_output = get_VGG19(input_size=(96,96,3))\n",
    "vgg_content = Model(vgg_input, vgg_output)\n",
    "#vgg_content.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9600 samples, validate on 2400 samples\n",
      "Epoch 1/20\n",
      "9600/9600 [==============================] - 83s 9ms/step - loss: 0.6421 - accuracy: 0.6546 - val_loss: 0.3423 - val_accuracy: 0.6533\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.34235, saving model to ./model_and_history/final3_perceptual_bi_4848_12000_subpixel_X2.h5\n",
      "Epoch 2/20\n",
      "9600/9600 [==============================] - 84s 9ms/step - loss: 0.3680 - accuracy: 0.6408 - val_loss: 0.3114 - val_accuracy: 0.6404\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.34235 to 0.31135, saving model to ./model_and_history/final3_perceptual_bi_4848_12000_subpixel_X2.h5\n",
      "Epoch 3/20\n",
      "9600/9600 [==============================] - 82s 9ms/step - loss: 0.3215 - accuracy: 0.6469 - val_loss: 0.2744 - val_accuracy: 0.6401\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.31135 to 0.27439, saving model to ./model_and_history/final3_perceptual_bi_4848_12000_subpixel_X2.h5\n",
      "Epoch 4/20\n",
      "9600/9600 [==============================] - 84s 9ms/step - loss: 0.2939 - accuracy: 0.6485 - val_loss: 0.2652 - val_accuracy: 0.6297\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.27439 to 0.26521, saving model to ./model_and_history/final3_perceptual_bi_4848_12000_subpixel_X2.h5\n",
      "Epoch 5/20\n",
      "9600/9600 [==============================] - 84s 9ms/step - loss: 0.2803 - accuracy: 0.6434 - val_loss: 0.2481 - val_accuracy: 0.6326\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.26521 to 0.24811, saving model to ./model_and_history/final3_perceptual_bi_4848_12000_subpixel_X2.h5\n",
      "Epoch 6/20\n",
      "9600/9600 [==============================] - 82s 9ms/step - loss: 0.2716 - accuracy: 0.6385 - val_loss: 0.2457 - val_accuracy: 0.6345\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.24811 to 0.24567, saving model to ./model_and_history/final3_perceptual_bi_4848_12000_subpixel_X2.h5\n",
      "Epoch 7/20\n",
      "9600/9600 [==============================] - 85s 9ms/step - loss: 0.2652 - accuracy: 0.6281 - val_loss: 0.2346 - val_accuracy: 0.6071\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.24567 to 0.23460, saving model to ./model_and_history/final3_perceptual_bi_4848_12000_subpixel_X2.h5\n",
      "Epoch 8/20\n",
      "9600/9600 [==============================] - 84s 9ms/step - loss: 0.2594 - accuracy: 0.6223 - val_loss: 0.2339 - val_accuracy: 0.6172\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.23460 to 0.23392, saving model to ./model_and_history/final3_perceptual_bi_4848_12000_subpixel_X2.h5\n",
      "Epoch 9/20\n",
      "9600/9600 [==============================] - 82s 9ms/step - loss: 0.2534 - accuracy: 0.6232 - val_loss: 0.2334 - val_accuracy: 0.6305\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.23392 to 0.23344, saving model to ./model_and_history/final3_perceptual_bi_4848_12000_subpixel_X2.h5\n",
      "Epoch 10/20\n",
      "9600/9600 [==============================] - 83s 9ms/step - loss: 0.2504 - accuracy: 0.6208 - val_loss: 0.2357 - val_accuracy: 0.6141\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.23344\n",
      "Epoch 11/20\n",
      "9600/9600 [==============================] - 79s 8ms/step - loss: 0.2460 - accuracy: 0.6144 - val_loss: 0.2247 - val_accuracy: 0.5991\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.23344 to 0.22474, saving model to ./model_and_history/final3_perceptual_bi_4848_12000_subpixel_X2.h5\n",
      "Epoch 12/20\n",
      "9600/9600 [==============================] - 83s 9ms/step - loss: 0.2426 - accuracy: 0.6114 - val_loss: 0.2379 - val_accuracy: 0.5730\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.22474\n",
      "Epoch 13/20\n",
      "9600/9600 [==============================] - 84s 9ms/step - loss: 0.2390 - accuracy: 0.6082 - val_loss: 0.2243 - val_accuracy: 0.5929\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.22474 to 0.22435, saving model to ./model_and_history/final3_perceptual_bi_4848_12000_subpixel_X2.h5\n",
      "Epoch 14/20\n",
      "9600/9600 [==============================] - 84s 9ms/step - loss: 0.2364 - accuracy: 0.6063 - val_loss: 0.2208 - val_accuracy: 0.5927\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.22435 to 0.22081, saving model to ./model_and_history/final3_perceptual_bi_4848_12000_subpixel_X2.h5\n",
      "Epoch 15/20\n",
      "9600/9600 [==============================] - 84s 9ms/step - loss: 0.2334 - accuracy: 0.6025 - val_loss: 0.2138 - val_accuracy: 0.6303\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.22081 to 0.21378, saving model to ./model_and_history/final3_perceptual_bi_4848_12000_subpixel_X2.h5\n",
      "Epoch 16/20\n",
      "9600/9600 [==============================] - 81s 8ms/step - loss: 0.2315 - accuracy: 0.5997 - val_loss: 0.2165 - val_accuracy: 0.5926\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.21378\n",
      "Epoch 17/20\n",
      "9600/9600 [==============================] - 84s 9ms/step - loss: 0.2289 - accuracy: 0.5961 - val_loss: 0.2182 - val_accuracy: 0.5899\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.21378\n",
      "Epoch 18/20\n",
      "9600/9600 [==============================] - 83s 9ms/step - loss: 0.2266 - accuracy: 0.5951 - val_loss: 0.2129 - val_accuracy: 0.6042\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.21378 to 0.21294, saving model to ./model_and_history/final3_perceptual_bi_4848_12000_subpixel_X2.h5\n",
      "Epoch 19/20\n",
      "9600/9600 [==============================] - 84s 9ms/step - loss: 0.2245 - accuracy: 0.5870 - val_loss: 0.2096 - val_accuracy: 0.6054\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.21294 to 0.20961, saving model to ./model_and_history/final3_perceptual_bi_4848_12000_subpixel_X2.h5\n",
      "Epoch 20/20\n",
      "9600/9600 [==============================] - 83s 9ms/step - loss: 0.2239 - accuracy: 0.5866 - val_loss: 0.2133 - val_accuracy: 0.5834\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.20961\n"
     ]
    }
   ],
   "source": [
    "# train bicubic X2 model\n",
    "\n",
    "model = final_model(48, 48, 3)\n",
    "model.compile(optimizer=Adam(lr=1e-4), loss=perceptual_loss_x2, metrics=['accuracy'])\n",
    "checkpointer = ModelCheckpoint(filepath='./model_and_history/final3_perceptual_bi_4848_12000_subpixel_X2.h5', verbose=1, \n",
    "                               monitor='val_loss', mode='auto', save_best_only=True)\n",
    "\n",
    "history = model.fit(LR_patch_train, HR_patch_train, epochs=20, verbose=1, \n",
    "                    batch_size=4, validation_split=0.2,\n",
    "                    callbacks=[checkpointer]\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9600 samples, validate on 2400 samples\n",
      "Epoch 1/20\n",
      "9600/9600 [==============================] - 99s 10ms/step - loss: 0.9243 - accuracy: 0.6804 - val_loss: 0.4832 - val_accuracy: 0.7495\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.48318, saving model to ./model_and_history/final3_perceptual_unknown_4848_12000_subpixel_X2.h5\n",
      "Epoch 2/20\n",
      "9600/9600 [==============================] - 91s 9ms/step - loss: 0.5052 - accuracy: 0.7440 - val_loss: 0.4290 - val_accuracy: 0.7593\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.48318 to 0.42897, saving model to ./model_and_history/final3_perceptual_unknown_4848_12000_subpixel_X2.h5\n",
      "Epoch 3/20\n",
      "9600/9600 [==============================] - 92s 10ms/step - loss: 0.4452 - accuracy: 0.7574 - val_loss: 0.3924 - val_accuracy: 0.7713\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.42897 to 0.39237, saving model to ./model_and_history/final3_perceptual_unknown_4848_12000_subpixel_X2.h5\n",
      "Epoch 4/20\n",
      "9600/9600 [==============================] - 88s 9ms/step - loss: 0.4109 - accuracy: 0.7693 - val_loss: 0.3733 - val_accuracy: 0.7780\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.39237 to 0.37328, saving model to ./model_and_history/final3_perceptual_unknown_4848_12000_subpixel_X2.h5\n",
      "Epoch 5/20\n",
      "9600/9600 [==============================] - 90s 9ms/step - loss: 0.3933 - accuracy: 0.7721 - val_loss: 0.3611 - val_accuracy: 0.7712\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.37328 to 0.36112, saving model to ./model_and_history/final3_perceptual_unknown_4848_12000_subpixel_X2.h5\n",
      "Epoch 6/20\n",
      "9600/9600 [==============================] - 89s 9ms/step - loss: 0.3808 - accuracy: 0.7735 - val_loss: 0.3449 - val_accuracy: 0.7767\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.36112 to 0.34487, saving model to ./model_and_history/final3_perceptual_unknown_4848_12000_subpixel_X2.h5\n",
      "Epoch 7/20\n",
      "9600/9600 [==============================] - 89s 9ms/step - loss: 0.3709 - accuracy: 0.7720 - val_loss: 0.3357 - val_accuracy: 0.7637\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.34487 to 0.33570, saving model to ./model_and_history/final3_perceptual_unknown_4848_12000_subpixel_X2.h5\n",
      "Epoch 8/20\n",
      "9600/9600 [==============================] - 88s 9ms/step - loss: 0.3624 - accuracy: 0.7658 - val_loss: 0.3302 - val_accuracy: 0.7772\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.33570 to 0.33023, saving model to ./model_and_history/final3_perceptual_unknown_4848_12000_subpixel_X2.h5\n",
      "Epoch 9/20\n",
      "9600/9600 [==============================] - 88s 9ms/step - loss: 0.3564 - accuracy: 0.7600 - val_loss: 0.3219 - val_accuracy: 0.7532\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.33023 to 0.32185, saving model to ./model_and_history/final3_perceptual_unknown_4848_12000_subpixel_X2.h5\n",
      "Epoch 10/20\n",
      "9600/9600 [==============================] - 89s 9ms/step - loss: 0.3502 - accuracy: 0.7539 - val_loss: 0.3311 - val_accuracy: 0.7719\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.32185\n",
      "Epoch 11/20\n",
      "9600/9600 [==============================] - 89s 9ms/step - loss: 0.3459 - accuracy: 0.7436 - val_loss: 0.3248 - val_accuracy: 0.7158\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.32185\n",
      "Epoch 12/20\n",
      "9600/9600 [==============================] - 89s 9ms/step - loss: 0.3411 - accuracy: 0.7407 - val_loss: 0.3150 - val_accuracy: 0.7767\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.32185 to 0.31504, saving model to ./model_and_history/final3_perceptual_unknown_4848_12000_subpixel_X2.h5\n",
      "Epoch 13/20\n",
      "9600/9600 [==============================] - 89s 9ms/step - loss: 0.3371 - accuracy: 0.7352 - val_loss: 0.3171 - val_accuracy: 0.7171\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.31504\n",
      "Epoch 14/20\n",
      "9600/9600 [==============================] - 90s 9ms/step - loss: 0.3332 - accuracy: 0.7281 - val_loss: 0.3118 - val_accuracy: 0.7516\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.31504 to 0.31183, saving model to ./model_and_history/final3_perceptual_unknown_4848_12000_subpixel_X2.h5\n",
      "Epoch 15/20\n",
      "9600/9600 [==============================] - 89s 9ms/step - loss: 0.3298 - accuracy: 0.7242 - val_loss: 0.3143 - val_accuracy: 0.7351\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.31183\n",
      "Epoch 16/20\n",
      "9600/9600 [==============================] - 90s 9ms/step - loss: 0.3274 - accuracy: 0.7188 - val_loss: 0.3070 - val_accuracy: 0.7120\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.31183 to 0.30703, saving model to ./model_and_history/final3_perceptual_unknown_4848_12000_subpixel_X2.h5\n",
      "Epoch 17/20\n",
      "9600/9600 [==============================] - 91s 9ms/step - loss: 0.3244 - accuracy: 0.7115 - val_loss: 0.3060 - val_accuracy: 0.7004\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.30703 to 0.30599, saving model to ./model_and_history/final3_perceptual_unknown_4848_12000_subpixel_X2.h5\n",
      "Epoch 18/20\n",
      "9600/9600 [==============================] - 89s 9ms/step - loss: 0.3209 - accuracy: 0.7120 - val_loss: 0.3150 - val_accuracy: 0.7224\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.30599\n",
      "Epoch 19/20\n",
      "9600/9600 [==============================] - 90s 9ms/step - loss: 0.3195 - accuracy: 0.7055 - val_loss: 0.2952 - val_accuracy: 0.7035\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.30599 to 0.29523, saving model to ./model_and_history/final3_perceptual_unknown_4848_12000_subpixel_X2.h5\n",
      "Epoch 20/20\n",
      "9600/9600 [==============================] - 89s 9ms/step - loss: 0.3169 - accuracy: 0.7014 - val_loss: 0.2976 - val_accuracy: 0.6883\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.29523\n"
     ]
    }
   ],
   "source": [
    "# train unknown X2 model\n",
    "\n",
    "model = final_model(48, 48, 3)\n",
    "model.compile(optimizer=Adam(lr=1e-4), loss=perceptual_loss_x2, metrics=['accuracy'])\n",
    "checkpointer = ModelCheckpoint(filepath='./model_and_history/final3_perceptual_unknown_4848_12000_subpixel_X2.h5', verbose=1, \n",
    "                               monitor='val_loss', mode='auto', save_best_only=True)\n",
    "\n",
    "history = model.fit(LR_patch_train, HR_patch_train, epochs=20, verbose=1, \n",
    "                    batch_size=4, validation_split=0.2,\n",
    "                    callbacks=[checkpointer]\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model and history\n",
    "import pickle\n",
    "\n",
    "with open('./model_and_history/final3_perceptual_unknown_4848_12000_subpixel_X2.pkl','wb') as f:\n",
    "    pickle.dump(history.history, f)\n",
    "\n",
    "#model.save_weights('./model_and_history/final_mse_unknown_4848_12000_subpixel_X4 _weights.hdf5')\n",
    "#model.save('./model_and_history/perceptual_baseline1_3232_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
