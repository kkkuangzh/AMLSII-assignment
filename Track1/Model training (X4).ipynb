{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model training (X4 upscaling)\n",
    "The X4 model is built on top of the previous X2 model by cascading a new X2 model to it. When the training of the previous X2 model is finished, the weights of the X2 model are kept as the first part of the X4 model, which only needs to learn the mapping from X2 to X4. In this notebook, 12000 training patches are extracted to train the X4 model at scale 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import config\n",
    "\n",
    "gpu_devices = config.experimental.list_physical_devices('GPU')\n",
    "\n",
    "for device in gpu_devices: config.experimental.set_memory_growth(device, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load training images from directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12000, 48, 48, 3)\n",
      "(12000, 192, 192, 3)\n",
      "(48, 48, 3)\n",
      "(192, 192, 3)\n"
     ]
    }
   ],
   "source": [
    "# load training images from directory\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "# LR_train_path = './datasets/DIV2K_train_LR_unknown/X4/'\n",
    "LR_train_path = './datasets/DIV2K_train_LR_bicubic/X4/'\n",
    "HR_train_path = './datasets/DIV2K_train_HR/'\n",
    "\n",
    "LR_train_imgs = []\n",
    "HR_train_imgs = []\n",
    "\n",
    "for path, subpath, files in os.walk(LR_train_path):\n",
    "    files.sort()\n",
    "    for i in files:\n",
    "        if i == '.DS_Store':\n",
    "            continue\n",
    "        img = Image.open(LR_train_path + i)\n",
    "        LR_train_imgs.append(np.asarray(img))\n",
    "\n",
    "for path, subpath, files in os.walk(HR_train_path):\n",
    "    files.sort()\n",
    "    for i in files:\n",
    "        if i == '.DS_Store':\n",
    "            continue\n",
    "        img = Image.open(HR_train_path + i)\n",
    "        HR_train_imgs.append(np.asarray(img)) \n",
    "\n",
    "print(len(LR_train_imgs))\n",
    "print(len(HR_train_imgs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess (patch extraction + normalization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12000, 48, 48, 3)\n",
      "(12000, 192, 192, 3)\n"
     ]
    }
   ],
   "source": [
    "# randomly extract pathches from training images (X4 upscaling)\n",
    "\n",
    "from extract_patches import *\n",
    "\n",
    "patch_height = 48\n",
    "patch_width = 48\n",
    "patch_num = 12000\n",
    "up_scale = 4\n",
    "\n",
    "LR_patch_train, HR_patch_train = train_patch(LR_train_imgs, HR_train_imgs, patch_height, patch_width, patch_num, up_scale)\n",
    "\n",
    "\n",
    "print(LR_patch_train.shape)\n",
    "print(HR_patch_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normaliza imgs from 0~255 to 0~1\n",
    "\n",
    "def normalize(imgs):\n",
    "    return imgs / 255\n",
    "\n",
    "HR_patch_train = normalize(HR_patch_train)\n",
    "LR_patch_train = normalize(LR_patch_train)\n",
    "\n",
    "print(LR_patch_train.shape)\n",
    "print(HR_patch_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load X2 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# define the perceptual_loss_x2 so that the X2 model can be loaded\n",
    "\n",
    "from keras.applications.vgg19 import VGG19\n",
    "from keras.layers import Input, Lambda\n",
    "from keras.models import Model\n",
    "import keras\n",
    "\n",
    "\n",
    "def get_VGG19(input_size):  \n",
    "    vgg_input = Input(input_size)\n",
    "    vgg = VGG19(include_top=False, input_tensor=vgg_inp)\n",
    "    for l in vgg.layers: \n",
    "        l.trainable = False\n",
    "    vgg_output = vgg.get_layer('block2_conv2').output \n",
    "    \n",
    "    return vgg_input, vgg_output\n",
    "\n",
    "def perceptual_loss_x2(y_true, y_pred):\n",
    "    \n",
    "    y_t = vgg_content1(y_true)\n",
    "    y_p = vgg_content1(y_pred)\n",
    "    loss = keras.losses.mean_squared_error(y_t, y_p)\n",
    "    \n",
    "    return loss\n",
    "\n",
    "vgg_input, vgg_output = get_VGG19(input_size=(96,96,3))\n",
    "vgg_content1 = Model(vgg_input, vgg_output)\n",
    "#vgg_content.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load trained X2 model\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.models import load_model\n",
    "\n",
    "x2_model = load_model('./models/final3_perceptual_unknown_4848_12000_subpixel_X2.h5', \n",
    "                      custom_objects={'tf': tf, 'perceptual_loss_x2':perceptual_loss_x2})\n",
    "#x2_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Network Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define subpixel layer\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.layers import Lambda\n",
    "\n",
    "def pixelshuffler(input_shape, batch_size, scale=2):\n",
    "    def subpixel_shape(input_shape=input_shape, batch_size=batch_size):\n",
    "        dim = [batch_size,\n",
    "               input_shape[1] * scale,\n",
    "               input_shape[2] * scale,\n",
    "               int(input_shape[3]/ (scale ** 2))]\n",
    "\n",
    "        output_shape = tuple(dim)\n",
    "\n",
    "        return output_shape\n",
    "\n",
    "    def pixelshuffle_upscale(x):\n",
    "        return tf.nn.depth_to_space(input=x, block_size=scale)\n",
    "\n",
    "    return Lambda(function=pixelshuffle_upscale, output_shape=subpixel_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model architecture\n",
    "\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import PReLU, Input, Conv2D, add\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "def res_block(inputs):\n",
    "    x = Conv2D(filters=64, kernel_size=(3, 3), strides=(1, 1), padding='same')(inputs)\n",
    "    x = PReLU(shared_axes=[1, 2])(x)\n",
    "    x = Conv2D(filters=64, kernel_size=(3, 3), strides=(1, 1), padding='same')(x)\n",
    "    return add([x, inputs])\n",
    "\n",
    "\n",
    "def final_model(patch_height, patch_width, channel, upscale=2):\n",
    "    # conv and then upsample\n",
    "    \n",
    "    inputs = Input(shape=(patch_height, patch_width, channel))\n",
    "    x_init = Conv2D(filters=64, kernel_size=(9, 9), strides=(1, 1), padding='same')(inputs)\n",
    "    x = PReLU(shared_axes=[1, 2])(x_init)\n",
    "    \n",
    "    # residual block\n",
    "    for i in range(8):\n",
    "        x = res_block(x)\n",
    "        \n",
    "    x = Conv2D(filters=64, kernel_size=(3, 3), strides=(1, 1), padding='same')(x)\n",
    "    x = add([x, x_init])\n",
    "    \n",
    "    # sub-pixel up_block    \n",
    "    x = Conv2D(filters=256, kernel_size=(3, 3), strides=(1, 1), padding='same')(x)\n",
    "    x = pixelshuffler(input_shape=(96,96,3), batch_size=4, scale=upscale)(x)\n",
    "    x = PReLU(shared_axes=[1, 2])(x)\n",
    "    \n",
    "    # output_block\n",
    "    output = Conv2D(filters=3, kernel_size=(9, 9), strides=(1, 1), padding='same')(x)\n",
    "    output = Conv2D(3, (1, 1), activation='sigmoid',padding='same')(output)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=output)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the perceptual_loss_x4 for compare X4 model output of size (192, 192)\n",
    "\n",
    "from keras.applications.vgg19 import VGG19\n",
    "from keras.layers import Input\n",
    "from keras.layers import Lambda\n",
    "import keras\n",
    "\n",
    "def get_VGG19(input_size):\n",
    "    \n",
    "    vgg_inp = Input(input_size)\n",
    "    vgg = VGG19(include_top=False, input_tensor=vgg_inp)\n",
    "    for l in vgg.layers: \n",
    "        l.trainable = False\n",
    "    vgg_outp = vgg.get_layer('block2_conv2').output \n",
    "    \n",
    "    return vgg_inp, vgg_outp\n",
    "\n",
    "def perceptual_loss_x4(y_true, y_pred):\n",
    "    \n",
    "    y_t = vgg_content2(y_true)\n",
    "    y_p = vgg_content2(y_pred)\n",
    "    loss = keras.losses.mean_squared_error(y_t, y_p)\n",
    "    \n",
    "    return loss\n",
    "\n",
    "vgg_input, vgg_output = get_VGG19(input_size=(192,192,3))\n",
    "vgg_content2 = Model(vgg_input, vgg_output)\n",
    "#vgg_content.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the latter part of the integrated model\n",
    "\n",
    "x4_model = final_model(96, 96, 3)\n",
    "#x4_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "model_2 (Model)              (4, 96, 96, 3)            807311    \n",
      "_________________________________________________________________\n",
      "model_3 (Model)              (4, 192, 192, 3)          807311    \n",
      "=================================================================\n",
      "Total params: 1,614,622\n",
      "Trainable params: 807,311\n",
      "Non-trainable params: 807,311\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# cascade two models to achieve progressive super-resolution\n",
    "# set the first part of model not trainable\n",
    "\n",
    "def integrated_network(base_model1, base_model2):\n",
    "    \n",
    "    base_model1.trainable = False\n",
    "\n",
    "    add_model = Sequential()\n",
    "    add_model.add(base_model1)\n",
    "    add_model.add(base_model2)\n",
    "    \n",
    "    return add_model\n",
    "\n",
    "model = integrated_network(x2_model, x4_model)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9600 samples, validate on 2400 samples\n",
      "Epoch 1/20\n",
      "9600/9600 [==============================] - 168s 18ms/step - loss: 1.4851 - accuracy: 0.6713 - val_loss: 1.2146 - val_accuracy: 0.7119\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.21457, saving model to ./model_and_history/final3_perceptual_bi_4848_12000_subpixel_X4.h5\n",
      "Epoch 2/20\n",
      "9600/9600 [==============================] - 159s 17ms/step - loss: 1.2908 - accuracy: 0.7484 - val_loss: 1.1804 - val_accuracy: 0.7561\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.21457 to 1.18041, saving model to ./model_and_history/final3_perceptual_bi_4848_12000_subpixel_X4.h5\n",
      "Epoch 3/20\n",
      "9600/9600 [==============================] - 159s 17ms/step - loss: 1.2614 - accuracy: 0.7587 - val_loss: 1.1592 - val_accuracy: 0.7377\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.18041 to 1.15921, saving model to ./model_and_history/final3_perceptual_bi_4848_12000_subpixel_X4.h5\n",
      "Epoch 4/20\n",
      "9600/9600 [==============================] - 160s 17ms/step - loss: 1.2440 - accuracy: 0.7652 - val_loss: 1.1405 - val_accuracy: 0.7754\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.15921 to 1.14045, saving model to ./model_and_history/final3_perceptual_bi_4848_12000_subpixel_X4.h5\n",
      "Epoch 5/20\n",
      "9600/9600 [==============================] - 157s 16ms/step - loss: 1.2330 - accuracy: 0.7691 - val_loss: 1.1357 - val_accuracy: 0.7617\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.14045 to 1.13569, saving model to ./model_and_history/final3_perceptual_bi_4848_12000_subpixel_X4.h5\n",
      "Epoch 6/20\n",
      "9600/9600 [==============================] - 160s 17ms/step - loss: 1.2226 - accuracy: 0.7650 - val_loss: 1.1270 - val_accuracy: 0.7838\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.13569 to 1.12697, saving model to ./model_and_history/final3_perceptual_bi_4848_12000_subpixel_X4.h5\n",
      "Epoch 7/20\n",
      "9600/9600 [==============================] - 159s 17ms/step - loss: 1.2152 - accuracy: 0.7694 - val_loss: 1.1264 - val_accuracy: 0.7751\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.12697 to 1.12642, saving model to ./model_and_history/final3_perceptual_bi_4848_12000_subpixel_X4.h5\n",
      "Epoch 8/20\n",
      "9600/9600 [==============================] - 161s 17ms/step - loss: 1.2084 - accuracy: 0.7703 - val_loss: 1.1294 - val_accuracy: 0.7908\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 1.12642\n",
      "Epoch 9/20\n",
      "9600/9600 [==============================] - 157s 16ms/step - loss: 1.2014 - accuracy: 0.7719 - val_loss: 1.1208 - val_accuracy: 0.7844\n",
      "\n",
      "Epoch 00009: val_loss improved from 1.12642 to 1.12084, saving model to ./model_and_history/final3_perceptual_bi_4848_12000_subpixel_X4.h5\n",
      "Epoch 10/20\n",
      "9600/9600 [==============================] - 162s 17ms/step - loss: 1.1963 - accuracy: 0.7698 - val_loss: 1.1234 - val_accuracy: 0.7562\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 1.12084\n",
      "Epoch 11/20\n",
      "9600/9600 [==============================] - 158s 16ms/step - loss: 1.1903 - accuracy: 0.7669 - val_loss: 1.1107 - val_accuracy: 0.7643\n",
      "\n",
      "Epoch 00011: val_loss improved from 1.12084 to 1.11067, saving model to ./model_and_history/final3_perceptual_bi_4848_12000_subpixel_X4.h5\n",
      "Epoch 12/20\n",
      "9600/9600 [==============================] - 164s 17ms/step - loss: 1.1854 - accuracy: 0.7700 - val_loss: 1.1103 - val_accuracy: 0.7677\n",
      "\n",
      "Epoch 00012: val_loss improved from 1.11067 to 1.11030, saving model to ./model_and_history/final3_perceptual_bi_4848_12000_subpixel_X4.h5\n",
      "Epoch 13/20\n",
      "9600/9600 [==============================] - 160s 17ms/step - loss: 1.1807 - accuracy: 0.7653 - val_loss: 1.1013 - val_accuracy: 0.7608\n",
      "\n",
      "Epoch 00013: val_loss improved from 1.11030 to 1.10135, saving model to ./model_and_history/final3_perceptual_bi_4848_12000_subpixel_X4.h5\n",
      "Epoch 14/20\n",
      "9600/9600 [==============================] - 161s 17ms/step - loss: 1.1758 - accuracy: 0.7684 - val_loss: 1.1035 - val_accuracy: 0.7624\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 1.10135\n",
      "Epoch 15/20\n",
      "9600/9600 [==============================] - 159s 17ms/step - loss: 1.1710 - accuracy: 0.7589 - val_loss: 1.1051 - val_accuracy: 0.7610\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 1.10135\n",
      "Epoch 16/20\n",
      "9600/9600 [==============================] - 161s 17ms/step - loss: 1.1670 - accuracy: 0.7577 - val_loss: 1.0966 - val_accuracy: 0.7788\n",
      "\n",
      "Epoch 00016: val_loss improved from 1.10135 to 1.09658, saving model to ./model_and_history/final3_perceptual_bi_4848_12000_subpixel_X4.h5\n",
      "Epoch 17/20\n",
      "9600/9600 [==============================] - 159s 17ms/step - loss: 1.1625 - accuracy: 0.7557 - val_loss: 1.0948 - val_accuracy: 0.7520\n",
      "\n",
      "Epoch 00017: val_loss improved from 1.09658 to 1.09476, saving model to ./model_and_history/final3_perceptual_bi_4848_12000_subpixel_X4.h5\n",
      "Epoch 18/20\n",
      "9600/9600 [==============================] - 162s 17ms/step - loss: 1.1584 - accuracy: 0.7519 - val_loss: 1.0985 - val_accuracy: 0.7298\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 1.09476\n",
      "Epoch 19/20\n",
      "9600/9600 [==============================] - 159s 17ms/step - loss: 1.1547 - accuracy: 0.7527 - val_loss: 1.0980 - val_accuracy: 0.7544\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 1.09476\n",
      "Epoch 20/20\n",
      "9600/9600 [==============================] - 160s 17ms/step - loss: 1.1507 - accuracy: 0.7456 - val_loss: 1.0921 - val_accuracy: 0.7441\n",
      "\n",
      "Epoch 00020: val_loss improved from 1.09476 to 1.09210, saving model to ./model_and_history/final3_perceptual_bi_4848_12000_subpixel_X4.h5\n"
     ]
    }
   ],
   "source": [
    "# train bicubic_X4 model\n",
    "\n",
    "model.compile(optimizer=Adam(lr=1e-4), loss=perceptual_loss_x4, metrics=['accuracy'])\n",
    "checkpointer = ModelCheckpoint(filepath='./model_and_history/final3_perceptual_bi_4848_12000_subpixel_X4.h5', verbose=1, \n",
    "                               monitor='val_loss', mode='auto', save_best_only=True)\n",
    "\n",
    "history = model.fit(LR_patch_train, HR_patch_train, epochs=20, verbose=1, \n",
    "                    batch_size=4, validation_split=0.2,\n",
    "                    callbacks=[checkpointer]\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9600 samples, validate on 2400 samples\n",
      "Epoch 1/20\n",
      "9600/9600 [==============================] - 171s 18ms/step - loss: 2.2272 - accuracy: 0.5997 - val_loss: 1.7930 - val_accuracy: 0.6252\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.79303, saving model to ./model_and_history/final3_perceptual_unknown_4848_12000_subpixel_X4.h5\n",
      "Epoch 2/20\n",
      "9600/9600 [==============================] - 160s 17ms/step - loss: 1.8569 - accuracy: 0.6247 - val_loss: 1.6923 - val_accuracy: 0.6684\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.79303 to 1.69226, saving model to ./model_and_history/final3_perceptual_unknown_4848_12000_subpixel_X4.h5\n",
      "Epoch 3/20\n",
      "9600/9600 [==============================] - 161s 17ms/step - loss: 1.7679 - accuracy: 0.6388 - val_loss: 1.6323 - val_accuracy: 0.6676\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.69226 to 1.63231, saving model to ./model_and_history/final3_perceptual_unknown_4848_12000_subpixel_X4.h5\n",
      "Epoch 4/20\n",
      "9600/9600 [==============================] - 159s 17ms/step - loss: 1.7146 - accuracy: 0.6525 - val_loss: 1.6052 - val_accuracy: 0.6672\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.63231 to 1.60518, saving model to ./model_and_history/final3_perceptual_unknown_4848_12000_subpixel_X4.h5\n",
      "Epoch 5/20\n",
      "9600/9600 [==============================] - 162s 17ms/step - loss: 1.6777 - accuracy: 0.6665 - val_loss: 1.5689 - val_accuracy: 0.6984\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.60518 to 1.56887, saving model to ./model_and_history/final3_perceptual_unknown_4848_12000_subpixel_X4.h5\n",
      "Epoch 6/20\n",
      "9600/9600 [==============================] - 159s 17ms/step - loss: 1.6502 - accuracy: 0.6728 - val_loss: 1.5617 - val_accuracy: 0.6581\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.56887 to 1.56174, saving model to ./model_and_history/final3_perceptual_unknown_4848_12000_subpixel_X4.h5\n",
      "Epoch 7/20\n",
      "9600/9600 [==============================] - 166s 17ms/step - loss: 1.6281 - accuracy: 0.6786 - val_loss: 1.5297 - val_accuracy: 0.6928\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.56174 to 1.52974, saving model to ./model_and_history/final3_perceptual_unknown_4848_12000_subpixel_X4.h5\n",
      "Epoch 8/20\n",
      "9600/9600 [==============================] - 163s 17ms/step - loss: 1.6100 - accuracy: 0.6784 - val_loss: 1.5232 - val_accuracy: 0.7144\n",
      "\n",
      "Epoch 00008: val_loss improved from 1.52974 to 1.52323, saving model to ./model_and_history/final3_perceptual_unknown_4848_12000_subpixel_X4.h5\n",
      "Epoch 9/20\n",
      "9600/9600 [==============================] - 161s 17ms/step - loss: 1.5932 - accuracy: 0.6830 - val_loss: 1.5244 - val_accuracy: 0.7131\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 1.52323\n",
      "Epoch 10/20\n",
      "9600/9600 [==============================] - 162s 17ms/step - loss: 1.5796 - accuracy: 0.6843 - val_loss: 1.5088 - val_accuracy: 0.7057\n",
      "\n",
      "Epoch 00010: val_loss improved from 1.52323 to 1.50882, saving model to ./model_and_history/final3_perceptual_unknown_4848_12000_subpixel_X4.h5\n",
      "Epoch 11/20\n",
      "9600/9600 [==============================] - 159s 17ms/step - loss: 1.5657 - accuracy: 0.6845 - val_loss: 1.4985 - val_accuracy: 0.7104\n",
      "\n",
      "Epoch 00011: val_loss improved from 1.50882 to 1.49848, saving model to ./model_and_history/final3_perceptual_unknown_4848_12000_subpixel_X4.h5\n",
      "Epoch 12/20\n",
      "9600/9600 [==============================] - 160s 17ms/step - loss: 1.5544 - accuracy: 0.6880 - val_loss: 1.4932 - val_accuracy: 0.7126\n",
      "\n",
      "Epoch 00012: val_loss improved from 1.49848 to 1.49320, saving model to ./model_and_history/final3_perceptual_unknown_4848_12000_subpixel_X4.h5\n",
      "Epoch 13/20\n",
      "9600/9600 [==============================] - 158s 16ms/step - loss: 1.5443 - accuracy: 0.6901 - val_loss: 1.4936 - val_accuracy: 0.6750\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 1.49320\n",
      "Epoch 14/20\n",
      "9600/9600 [==============================] - 165s 17ms/step - loss: 1.5364 - accuracy: 0.6885 - val_loss: 1.4872 - val_accuracy: 0.6865\n",
      "\n",
      "Epoch 00014: val_loss improved from 1.49320 to 1.48718, saving model to ./model_and_history/final3_perceptual_unknown_4848_12000_subpixel_X4.h5\n",
      "Epoch 15/20\n",
      "9600/9600 [==============================] - 160s 17ms/step - loss: 1.5278 - accuracy: 0.6898 - val_loss: 1.4630 - val_accuracy: 0.6857\n",
      "\n",
      "Epoch 00015: val_loss improved from 1.48718 to 1.46301, saving model to ./model_and_history/final3_perceptual_unknown_4848_12000_subpixel_X4.h5\n",
      "Epoch 16/20\n",
      "9600/9600 [==============================] - 161s 17ms/step - loss: 1.5210 - accuracy: 0.6908 - val_loss: 1.4641 - val_accuracy: 0.6811\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 1.46301\n",
      "Epoch 17/20\n",
      "9600/9600 [==============================] - 160s 17ms/step - loss: 1.5125 - accuracy: 0.6922 - val_loss: 1.4554 - val_accuracy: 0.6796\n",
      "\n",
      "Epoch 00017: val_loss improved from 1.46301 to 1.45545, saving model to ./model_and_history/final3_perceptual_unknown_4848_12000_subpixel_X4.h5\n",
      "Epoch 18/20\n",
      "9600/9600 [==============================] - 160s 17ms/step - loss: 1.5053 - accuracy: 0.6938 - val_loss: 1.4566 - val_accuracy: 0.6842\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 1.45545\n",
      "Epoch 19/20\n",
      "9600/9600 [==============================] - 161s 17ms/step - loss: 1.5008 - accuracy: 0.6966 - val_loss: 1.4579 - val_accuracy: 0.6834\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 1.45545\n",
      "Epoch 20/20\n",
      "9600/9600 [==============================] - 159s 17ms/step - loss: 1.4958 - accuracy: 0.6962 - val_loss: 1.4501 - val_accuracy: 0.7053\n",
      "\n",
      "Epoch 00020: val_loss improved from 1.45545 to 1.45014, saving model to ./model_and_history/final3_perceptual_unknown_4848_12000_subpixel_X4.h5\n"
     ]
    }
   ],
   "source": [
    "# train unknown X4 model\n",
    "\n",
    "model.compile(optimizer=Adam(lr=1e-4), loss=perceptual_loss_x4, metrics=['accuracy'])\n",
    "checkpointer = ModelCheckpoint(filepath='./model_and_history/final3_perceptual_unknown_4848_12000_subpixel_X4.h5', verbose=1, \n",
    "                               monitor='val_loss', mode='auto', save_best_only=True)\n",
    "\n",
    "history = model.fit(LR_patch_train, HR_patch_train, epochs=20, verbose=1, \n",
    "                    batch_size=4, validation_split=0.2,\n",
    "                    callbacks=[checkpointer]\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model and history\n",
    "import pickle\n",
    "\n",
    "with open('./model_and_history/final3_perceptual_unknown_4848_12000_subpixel_X4.pkl','wb') as f:\n",
    "    pickle.dump(history.history, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
